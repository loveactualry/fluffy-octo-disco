---
output:
  pdf_document: default
  html_document: default
  word_document: default
---

# 5장 빅데이터 정제

## 1. 분석용 데이터 세트 정제

### 1. 데이터 정제 절차



#### 1. 데이터 오류



* 결측치, 잡음 이상치 등이 포함\

  + 정제 과정 필수 \
  
  
* 데이터 오류 주 요소\

  + 결측치, 잡음, 이상치, 측정 오차 오류\
  
  + 측정오차 오류: 소재, 시공간 제약, 측정, 피 측정자의 사고 판단력 오류\
  
* 결측치\

  + 측정 샘플에서 누락된 변수 값을 의미\
  
    - 측정을 원하지 않는 경우\
    
  + 샘플 제거, 해당 변수 제거, 결측치 무시, 결측치 추정 등을 이용 \
  
* 잡음\

  + 데이터 측정 시, 다양한 사유로 개입되는 임의의 요소\
  
  + 변수값을 본래의 참값에서 벗어나게 하는 요소\
  
  + 측정을 잘못하게 만드는 요소로 이해\

  + **잡음 포함 데이터는 잘못된 분석 결과로 이어질 수 있다.**\

  + *예시 - 대학생 스마트폰 사용 추이 분석*  \
  
    - 통신 오류로 인한 사용량 측정 오류 \
    
    - 수집자 입력 오류 \
    
  + 구간화, 군집화, 회귀 모형 변환등의 방법으로 잡음 제거
  
  + 구간화\
  
    - **연속 변수** 를 다수의 구간으로 나눈 후, 동일 구간 변수값을 하나의 변수로 변환\
    
  + 군집화\
    
    - **데이터 집합**을 수개의 군집으로 묶은 후, 동일 군집 데이터를 대표값으로 치환하는 방법\
    
  
  +회귀모형\
  
    
    - 회귀 모형 추정하여 모형에 있는 변수값으로 변환\
  
  
    - 평활(smoothing) - 직선 회귀 모형을 사용한 예\
  
  * 이상치\
  
    - 다른 측정값들과 현저한 차이를 보이는 샘플 혹은 변수값\
    
    - 단순 오류 혹은 정상 측정 특이값
    
    - 사분위수 분석에는 큰 차이 없음
    
    - 범위, 분산, 표준편차에 큰 영향 
    
    - 제거 혹은 무시 하거나 관심을 갖고 분석 수행
    
  > 빅데이터 정제: 데이터 불완전 요소 제거 과정
  
  
#### 2. 데이터 정제 절차 

* 데이터 품질 확인과 점검 

  + 수집 데이터 변환
  
  + 수집 데이터 저장
  
  + 저장 데이터 활용
    
* 원시데이터 -> 분석 데이터 구조로 정제

> 빅데이터 정제 그림 참조

* 빅데이터 처리 과정

  + 데이터로 부터 유용한 정보 탐색
  
  + 의사걸정 지원
  
  + 점검, 정제, 변환, 모델링 및 분석 프로세스를 전반적으로 포함
  
  + *데이터 양 증가로 다양한 이슈 발생*
  
* 빅데이터 처리 요소

  + 통계 분석
  
  + 데이터 마이닝
  
* 빅데이터 처리 시스템 

  + 여러 저장 시스템과 연계를 고려하여 구성
  
    - 실시간
    - 배치
    - 인메모리
  
  + 개별 구축과 클라우드 컴퓨팅 사용방식 고려
  
    - 처리 시스템 구축을 위한 하드웨어 군 설치
    
#### 3. 빅데이터 분석 도구 

* 처리 데이터 표현에 따라 다양한 분석결과를 얻는다.

* 데이터 분석 도구

  + Hadoop
  
    - 자바 기반 오픈소스 프레임 워크
    
    - 대량 자료 처리
    
    - 분산 응용 프로그램 지원
    
  + MapReduce
  
    - Map 함수와 Reduce 함수 기반 구성되는 (key, value) 작업 병렬 처리 
    
    - Map: 흩어져 있는 데이터를 연관성 있는 데이터 들로 분류
    
    - Reduce: Map 출력 데이터 중복 제거 후, 데이터 추출
    
    ![haddop_int](C:\Users\loveactualry\Pictures\hadoopint.PNG)
    
    
  + R 
    
    - 통계 소프트웨어 
    
  + Presto
  
    - Facebook For Hadoop SQL
    
    - SQL 언어 사용
    
    - 클라우데라 , 임팔라, 아파치 타조 등과 유사
    
  + BigQuery
  
    - Google, API 사용
    
    - 분석 데이터 구글 업로드, 2TB, 무료 사용
    
  + Summingbird
  
    - Storm과 Hadop 결합
    
    - 배치 및 스트리밍 작업에서 애플리케이션 수행
    
  + Esper
  
    - 실시간 처리 인메모리 기술
    
    - 여러 소스 발생 이벤트 데이터 핸들링
    
    - 추출 데이터 작업 수행
    
    
### 2. 데이터 정제 기술



#### 1. 데이터 정제

* 데이터 정제

  + 비정형 데이터 -> 정형 데이터 구조로 변화 
  
  + 값이 없거나 오류가 있으면 수정
  
* Legacy Data 통합 변환

  + 데이터 변환
  
    - 분석 용이한 형태로 변환
    
    - ETL을 통한 동일 형태 변환
    
  + 데이터 교정
  
    - 결측치 변환, 이상치 제거, 노이즈 데이터 교정
    
    - 비정형 데이터 수집 시 반드시 수행
    
  + 데이터 통합
  
    - 유사 기존 데이터와 연계 또는 통합
    
    - Legacy system 데이터와 통합하는 경우
    
* ETL 주요 기능

  + Extract, Transform, Load
  
    - 책 표 참고

* 데이터 교정

  + 데이터 결여, 노이즈가 있을 시 수정
  
  + 데이터 결여 시 -> 무시, 자동 채움, 별도 입력 결정
  
* 데이터 통합

  + 구조 통일
  
  + 중복, 기준 ,단위 점검 수정 반영

#### 2. 빅데이터 처리 방식 및 주요 솔루션

* 대화형 처리

  + 대용량 데이터 이용, 질의 답 빠르게 얻음
  
  + 서비스 BI 대시보드 형태 제공
  
  + 하이브 쉘, 임팔라 피그 대화형 모드
  + Redshift, Presto, Impala, Spark, Hive, Pig 등
  
* 배치 처리

  + 주기적 작업 
  
  + 일정 시간 소요
  + MapReduce, Hive, Pig, Spark
  
* 실시간 처리

  + 이벤트성 응답, 준 실시간 처리에 사용
  
  + 결제, 비정상 카드 사용 등에 대한 데이터 분석
  
* 주요 솔루션


  + Apache SW, Cloudera, Hortonworks, MapR, Azure, AWS
  


#### 3. 빅데이터 정제 처리 절차

* 세부 계획 수립

  + 데이터 분석 목적 정의
  + 시스템 환경 및 방법론 검토
  + 데이터 크기, 생성속도, 빈도 고려
  + 데이터 정제 및 처리 분석
  + 세부 시나리오 작성
  

* 자체 구축 및 클라우드 컴퓨팅 고려

  + 구축, 운영 방식 고려
  + 데이터 관리, 보안, 확장성 고려
  + 데이터 품질 서비스 수준 정의
  + 복합 방식, 장단점 고려
  
* 최적화 분석 플랫폼 구축

  + 부하 고려(용량, 분석 작업)
  + 클러스터 노드 및 인프라 spec 구축
  + 고용량 램 고려
  
* 클라우드 컴퓨터 계정 생성

  + 클라우드 환경 생성
  
#### 4. MapReduce를 이용한 데이터 정제, 처리

* 맴리듀스 프레임 워크

  + Map + Reduce
  
  + Map
  
    - 분산 데이터와 키값을 리스트로 모으는 단계
    
  + Reduce
  
    - 리스트에서 원하는 데이터를 찾아 처리하는 단계
    
  + **shuffle and sort** 작업 수행
  
  + 맵 생성 결과 로컬 디스크 저장, HDFS에 블록 형태 저장

  + 레코드 단위 데이터 처리 -> 노드에 작업 분배 후 병렬 수행 
  
  + 노드 데이터 처리, 결과 저장 
  
  
* 맴리듀스를 이용한 데이터 처리 

  + 하나의 mapper 프로그램은 HDFS로부터 하나의 입력 split 처리
  
  + 하나의 record를 맵퍼 프로그램에 전달
  
  + 맵퍼 중간 결과는 현대 노드 로컬 디스크에 저장 하고 리튜서에 전달
  
  + 분산 병렬 처리, 여러 작업 노드에 분산 수행 프레임워크 제공
  
* Mapper

  + Map Task는 일반적으로 하나의 HDFS 블록에서 수행
  
  + 맵태스크 블록 저장 노드에서 수행
  
  + 중간 결과 로컬 디스크에 저장
  

* Shuffle Sort

  + **모든 Mapper** 중간 데이터 정렬 리듀서로 전달
  
  + 맴퍼와 리듀서 중간에서 맴태스크 수행
  
  + 리듀서 태스크 시작 전 수행
  
  + 동일 키값 동일 리듀서 전달
  
  + 중간 키, 값 리스트는 정렬되어 리듀서로 전달 
  
* Reducer
  
  + 셔플과 정렬된 중간 결과를 받아 최종 결과 수행 
  
  + 리듀서 수행 최종 결과 HDFS에 저장
  
  
* Mapper와 Reducer는 프로그래머에 의해 작성된 코드로 처리, 셔플과 정렬은 맵리듀스 프레임워크에서 자동 수행

* Mapper와 Reducrer 클래스 정의, Driver 클래스는 작업에 대한 설정 정보 작성


#### 5. Pig를 이용한 데이터 정제, 처리

* 데이터 배치 처리를 위해 사용

* 대규모 데이터 세트 샘플 데이터 추출, ETL 작업 수행, 데이터 탐색

* 프로그래밍 기능 제공, Pig Latin 데이터 플로우 제어 언어 사용 

* Pig를 이용한 데이터 처리

  + 사용자 입력 pig Latin 문장 해석
  
  + 수행 결과는 출력 요청시 까지는 보여주지 않음
  
  + 순서대로 문장 수행
  
  + DUMP(STORE) 명령어로 결과 출력, 저장
  
  + 식별자는 필드와 다른 데이터 구조
  
  + int, long, float, double, boolean, datetime, chararray, bytearray
  
  + 데이터 정의
  
    - field: 하나의 데이터 요소 정의 
    
    - tuple: 필드의 묶음
    
    - bag: tuple의 묶음
    
    - realtion: 저장된 값을 갖는 간단한 bag
    
    - 하나의 피그 라틴 문장은 하나의 새로운 realtion 생성
    
  +  주요 기능
  
    - describe, filter, foreach, distinct, order, limit, trim, upper, random,
    
    - group, all flatten
    
  + 인터프리터
  
    - 전처리 수행 - 피르 라틴 파싱
    
    - 데이터 형식 검증, 최적화 작업 및 실행
    
    - 맴 리듀스 작업, 하둡 제출

  
### 3. 데이터 세분화



#### 1. 데이터 세분화

* 데이터 효과적 관리 필요

  + 수집한 빅데이터를 유용하게 활용하기 위함
  
* 검색, 수집 데이터를 분석에 사용하기 좋은 방식으로 보관 

  + 대용량 데이터를 고성능 저장, 검색, 수정 삭제 등의 서비스 제공
  
* 유의미한 기준에 따라 나누는 작업

  + 반정형, 비정형 데이터 형식 변화 기준에 따라 세분화 작업 필요 가능성
  
#### 2. 데이터 유형별 세분화

* 정형, 비정형, 반정형 구분

* 비정형, 반정형 데이터 작업 필요

* 비정형 데이터

  + 형태와 구조 복잡하여 기존 데이터베이스 저장 불가 
  
  + 대량 텍스트, 이미지 등
  
  + 정형데어터 변환하여 분석
  
* 텍스트의 경우 

  + 정형화 데이터 변환 후 분석 수행
  
* 이미지의 경우

   + 픽셀을 수치로 변환하는 과정을 거침, CNN 사용
   
* 반정형 데이터의 경우

  + 데이터 구조 일관성 없음
  
  + 정형화 데이터 제외하고 나머지 데이터를 정형화 데이터로 변환하여 저장 관리
  
  + XML, JSON, 등으로 되어 있음
  
  + JSON은 자바 스크리브 구문 형식을 따름
  
  + HTML은 웹페잊 고안 언어 링크, 인용등을 통해 구조적 문서 작성 가능
  
#### 3. 빅데이터 전처리

* 빅데이터 전처리와 후처리 업무로 구분

* 빅데에터 전처리

  + 수집 데이터 필터링
  
  + 데이터 유형 변환
  
  + 정체
  
  + 이후 저장소에 적재
  
* 필터링 

  + 필터링으로 제거하여 분석
  
  + 비정형 데이터는 데이터 마이닝을 통해 개선
  
* 데이터 유형 변환 

  + 데이터 유형 변환
  
* 정제

  + 수집 데이터 불일치성 교정
  
  + 결측값 처리, 이상치 및 잡음 제거
  
#### 4. 빅데이터 후처리

* 변환, 통합, 축소 등의 과정

* 변환

  + 분석에 용이하게 일관성 있는 형식으로 변환
  
  + 평활화, 집계, 일반화, 정규화, 속성생성
  
* 통합

  + 상호 연관성이 있는 데이터들을 하나로 결합
  
  + 연관관계 분석을 이용하여 중복 데이터 검출, 표현 단위 다른 항목 일치
  
* 축소

  + 분석 불필요 데이터 축소, 고유 특성은 손상되지 않으며 효율성 높이는 과정 진행
  
  
## 2. 데이터 오류 파악 및 수정

### 1. 데이터 결측값 처리



#### 1. 결측값의 종류

* 결측값, 이상치, 오입력 등의 문제 존재

  + 바로 분석을 못 할 수도 있음
  
* 결측값

  + 입력이 누락된 값
  
  + NA로 출력
  
  + 분석 과정에서 문제가 생기기 때문에 처리 해야 함
  
* 완전 무작위 결측(MCAR: Missing Completely At Random)

  + 어떤 변수의 결측치가 다른 변수와 아무 연관이 없는 경우
  
  + 이 경우 단순 무작위 표본 추출 하여 완벽 데이터 구성 가능(대규모 데이터 세트)
  
* 무작위 결측(MAR: Missing At Random)

  + 어떤 변수의 결측치가 다른 변수가 연관이 되어 있지만 비 관측 값들과는 연관이 되어있지 않은 경우
  
* 비무작위 결측(NMAR: Not Missing At Random)

  + 어떤 변수의 결측치가 완전 무작위 또는 무작위 결측이 아닌 경우
  
  + 소득 조사 예시
  
  
#### 2. 결측값 보완 방법

* 완전 무작위, 무작위 결측이라 가정

* 결측 데이터 삭제

  + 수집 데이터가 충분한 경우
  
  + 한두개의 변수가 50%이상 결측 자료 인 경우
  
* 결측 데이터 대체

  + 보완 방법에 따라 오차 발생 가능성
  
  + 평균치 삽입법
  
    - 변수 평균치 계산하여 사용
    
    - 오차 무작위 분포 혹은 50% 이상 미수집 자료가 있는 데이터가 삭제 된 경우
    
    - 데이터, 변수에 대한 사전 지식이 충분하지 않은 경우 활용
    
  + 보삽법
  
    - 시계열 자료 누락 데이터 보완
    
    - 심한 변동을 나타내지 않는 변수 추정 시 유용
    
    - 맥락적 사정 평가 고려
    
  + 평가치 추정법
  
    - 작은 오차 감수 
    
    - 맥락적 사정이나 행렬식 자료 고려하여 추정
    

* 보삽법 사용 맥락적 사정 및 평가에 의한 방법

  - 집단의 특징과 성격을 고려하여 일반적 지식의 견지에서 평가를 내리는 방법
  
* 평가치 추정법에서 행렬식 자료를 이용하는 경우

  - 결측 자료 행렬식 근거한 평가 반영하여 추정
  
  - 다회적 분석, 요인 분석 기법 필요
  
* 맥락적 자료를 수집하고 데이터 분석 


### 2. 데이터 이상값 처리



#### 1. 이상값

* 이상값은 입력 오류, 데이터 처리 오류 등의 이유로 발생하는 경우

* 특정 범위를 벗어나 모델링 결과에 영향을 미치는 경우

* 이상값 제거 과정을 전처리 과정에서 수행

#### 2. 이상값 검출 방법

<교재와 내용이 같은 출처>
[link](https://pubdata.tistory.com/52)



* Varinace: 정규분포에서 2.5% ~ 97.5% 포함값

* Likelihood: 베이즈 정리에 의해 데이터 세트가 가지는 두가시 샘플에 대한 발생 확률로 판별 ???????????

* Nearest-neighbor : 모든 데이터 쌍의 거리를 계산하여 이상치 검출

* Density : 샘플의 LOF(local outlier factor)를 계산하여 값이 가장 큰 데이터를 이상치로 추정, 밀도있는 데이터 셋으로 부터 먼 데이터

* Clustering : 데이터를 여러 클러스터로 구분한 후 작은 크기의 클러스터나 클러스터사이의 거리를 계산하여 먼 경우 해당 클러스터를 이상치로 판별 

#### 3. 이상치 처리

* 하한값 상한값 범위 내 대체

* 평균 표준 편차 대체

* 편균 절대 편차 대체

* 백분위수 대체


  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

  
